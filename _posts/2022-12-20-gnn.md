---
layout: post
usemathjax: true
title: "Exploring Graph Neural Networks (GNN)"
subtitle: "Applications in Python Package Analysis"
date: 2023-12-14
background: '/img/posts/gnn/network.jpeg'
---

<h1 style="text-align: center;">Introduction to Graph Neural Networks (GNN)</h1>

Graph Neural Networks (GNN) represent a significant advancement in the field of machine learning and data science, particularly for data structured in graph form. They have gained popularity for their ability to model complex relationships and interdependencies within data. This post delves into the intricacies of GNNs, focusing on their application in Python package analysis - a domain where dependencies and interconnections are abundant.

<h2 style="text-align: center;">Core Concepts of GNN</h2>

Graph Neural Networks are distinct from traditional neural networks due to their ability to process graph structures directly. This is crucial in scenarios where data points (nodes) have explicit relationships (edges), as in social networks, biochemical structures, or software packages.

GNNs operate by aggregating information from a node's neighbors, iteratively updating their representations. This process allows GNNs to capture both local structures and global graph properties, making them highly effective for tasks like node classification, link prediction, and graph classification.

<h3 style="text-align: center;">Types of GNN Tasks</h3>

GNNs can be categorized based on the level at which they operate:

1. **Node Classification:** This task involves predicting properties or classes of individual nodes. For instance, in a social network, it might involve classifying users based on their activities or connections.
2. **Edge Classification:** Here, the goal is to predict properties or the existence of edges between nodes. In recommendation systems, this could mean predicting whether a user would like a particular item.
3. **Graph Classification:** This involves understanding and predicting properties of entire graphs. In chemical informatics, this might mean predicting the properties of a molecule.

<h3 style="text-align: center;">Mathematics Behind GNN</h3>

GNNs use a process known as 'message passing', where nodes aggregate and transform feature information from their neighbors. This iterative process allows nodes to gather information from a broader neighborhood over successive layers.

Mathematically, this can be expressed as:

$$
h_v^{(l+1)} = f\left(h_v^{(l)}, \bigoplus_{u \in \mathcal{N}(v)} g(h_u^{(l)}, h_v^{(l)}, e_{uv})\right)
$$

Where:
- $$h_v^{(l)}$$ is the feature vector of node $$v$$ at layer $$l$$.
- $$\mathcal{N}(v)$$ is the set of neighbors of node $$v$$.
- $$f$$ and $$g$$ are neural network functions.
- $$e_{uv}$$ is the edge feature from node $$u$$ to node $$v$$.
- $$\bigoplus$$ denotes an aggregation operation, like sum, mean, or max.

<h2 style="text-align: center;">Application: Python Package Analysis</h2>

In the experiment, we used GNNs to map the complex landscape of Python packages, where packages are nodes, and their dependencies form the edges.

<h3 style="text-align: center;">Data Collection and Preprocessing</h3>

The first step was to gather data on Python packages from the Python Package Index (PyPI). We then preprocessed this data to construct a graph where nodes represent individual packages and edges represent dependencies between them.

```python
def fetch_package_data() -> List[Dict[str, Any]]:
    """
    Fetches package data from PyPI.

    Returns:
        List[Dict[str, Any]]: A list of dictionaries containing package information.
    """
    package_list = []
    BASE_URL = (
        "https://pypi.org/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3&page="
    )
    DEPENDANCY_URL = "https://pypi.org/pypi/{}/json"

    for i in tqdm(range(0, 502)):
        try:
            fp = urllib.request.urlopen(BASE_URL + str(i))
        except urllib.error.HTTPError as err:
            print(f"\n{BASE_URL + str(i)}, {err}")
            break

        mybytes = fp.read()
        mystr = mybytes.decode("utf8")
        fp.close()
        soup = BeautifulSoup(mystr, "html.parser")
        items = soup.findAll("a", {"class": "package-snippet"})

        for child_soup in items:
            package_name = child_soup.find(
                "span", {"class": "package-snippet__name"}
            ).get_text()
            description = child_soup.find(
                "p", {"class": "package-snippet__description"}
            ).get_text()
            json = requests.get(DEPENDANCY_URL.format(package_name)).json()

            if "message" in json and json["message"].lower() == "not found":
                continue

            dependancies = (
                [re.split("[^a-zA-Z-_]", x)[0] for x in json["info"]["requires_dist"]]
                if json["info"]["requires_dist"]
                else []
            )

            package_list.append(
                {
                    "name": package_name,
                    "description": description,
                    "dependancies": dependancies,
                    "topic": [
                        x.split("::")[1].strip()
                        for x in json["info"]["classifiers"]
                        if x.lower().startswith("topic")
                    ],
                    "intended audience": [
                        x.split("::")[-1].strip()
                        for x in json["info"]["classifiers"]
                        if x.lower().startswith("intended audience")
                    ],
                }
            )

    return package_list

```

This process involved extracting package metadata, including descriptions and lists of dependencies, which served as the foundation for our graph.

<h3 style="text-align: center;">Model Architecture</h3>
We designed a GNN model with several Graph Convolutional Network (GCN) layers, suitable for node classification tasks. The GCN layers enable the model to learn node representations by aggregating information from their neighbors.

```python
class GNN(torch.nn.Module):
    """
    Graph Neural Network model using GCNConv layers.

    Args:
        hidden_channels (int): Number of hidden channels.
        num_topics (int): Number of topics.
        num_audiences (int): Number of audiences.
    """

    def __init__(self, hidden_channels: int, num_topics: int, num_audiences: int):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(768, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.out_topic = torch.nn.Linear(hidden_channels, num_topics)
        self.out_audience = torch.nn.Linear(hidden_channels, num_audiences)

    def forward(
        self, x: torch.Tensor, edge_index: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass of the GNN.

        Args:
            x (torch.Tensor): Node feature matrix.
            edge_index (torch.Tensor): Graph edge index.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: Topic and audience output tensors.
        """
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        return self.out_topic(x), self.out_audience(x)

```

The model aimed to classify nodes (packages) based on features derived from their descriptions and dependency relationships.

<h3 style="text-align: center;">Training and Optimization</h3>
I trained our GNN model using a dataset split into training, validation, and test sets. The training process involved optimizing a loss function that measured the accuracy of our model's predictions against known classifications.

```python
def train_gnn(
    global_data: Data, num_topics: int, num_audiences: int, num_epochs: int = 100
) -> GNN:
    """
    Trains a Graph Neural Network (GNN) using the provided global data.

    Parameters:
    - global_data (Data): Graph data object containing node features, edge indices, and topic and audience labels.
    - num_topics (int): Number of unique topics.
    - num_audiences (int): Number of unique intended audiences.
    - num_epochs (int, optional): Number of training epochs. Default is 100.

    Returns:
    GNN: Trained GNN model.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    gnn_model = GNN(
        hidden_channels=128, num_topics=num_topics, num_audiences=num_audiences
    ).to(device)
    optimizer = Adam(gnn_model.parameters(), lr=0.01)
    criterion = BCEWithLogitsLoss()

    global_data = global_data.to(device)

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        out_topic, out_audience = gnn_model(global_data.x, global_data.edge_index)

        loss_topic = criterion(
            out_topic[global_data.train_mask],
            global_data.y_topic[global_data.train_mask],
        )
        loss_audience = criterion(
            out_audience[global_data.train_mask],
            global_data.y_audience[global_data.train_mask],
        )

        loss = loss_topic + loss_audience
        loss.backward()
        optimizer.step()

    return gnn_model
```

I also employed techniques like dropout and regularization to prevent overfitting and enhance the model's generalization capabilities.

<h3 style="text-align: center;">Model Evaluation</h3>
After training, i evaluated the model's performance on unseen data. We used metrics such as accuracy, precision, and recall to assess how well our model could classify packages based on their dependencies.

```python
def evaluate_model(model, data: Data) -> None:
    """
    Evaluate a GNN model on the given data.

    Parameters:
    - model: The trained GNN model.
    - data (Data): Graph data object containing node features, edge indices, and topic and audience labels.

    Returns:
    None
    """
    model.eval()
    with torch.no_grad():
        out_topic, out_audience = model(data.x, data.edge_index)

        # Convert the logits to probabilities
        out_topic_prob = torch.sigmoid(out_topic)
        out_audience_prob = torch.sigmoid(out_audience)

        # Convert probabilities to binary predictions
        out_topic_pred = (out_topic_prob > 0.5).float()
        out_audience_pred = (out_audience_prob > 0.5).float()

        # Compute accuracy and F1-score for 'topic'
        acc_topic = accuracy_score(
            data.y_topic[data.test_mask].cpu(), out_topic_pred[data.test_mask].cpu()
        )
        f1_topic = f1_score(
            data.y_topic[data.test_mask].cpu(),
            out_topic_pred[data.test_mask].cpu(),
            average="micro",
        )

        # Compute accuracy and F1-score for 'audience'
        acc_audience = accuracy_score(
            data.y_audience[data.test_mask].cpu(),
            out_audience_pred[data.test_mask].cpu(),
        )
        f1_audience = f1_score(
            data.y_audience[data.test_mask].cpu(),
            out_audience_pred[data.test_mask].cpu(),
            average="micro",
        )

        print(f"Topic - Accuracy: {acc_topic}, F1-score: {f1_topic}")
        print(f"Audience - Accuracy: {acc_audience}, F1-score: {f1_audience}")
```

<h3 style="text-align: center;">Insights and Visualization</h3>
To gain insights from our model, we visualized the graph, highlighting the intricate web of dependencies among Python packages. This visualization helped us understand community structures and key nodes (packages) within the graph.

```python
def plot_graph(edge_index, package_list: List[Dict[str, str]]) -> None:
    """
    Plot a graph using NetworkX and Plotly with the given edge index and package list.

    Parameters:
    - edge_index: Edge indices of the graph.
    - package_list (List[Dict[str, str]]): List of dictionaries representing packages, each containing 'name'.

    Returns:
    None
    """
    G = nx.Graph()
    edge_list = edge_index.t().tolist()
    G.add_edges_from(edge_list)

    # Create a mapping of node indices to package names
    index_to_package = {
        idx: package["name"] for idx, package in enumerate(package_list)
    }

    # Get positions for the nodes in G
    pos = nx.spring_layout(G)

    # Extract x and y coordinates
    x_nodes = [pos[k][0] for k in G.nodes()]
    y_nodes = [pos[k][1] for k in G.nodes()]

    # Create edges
    edge_x = []
    edge_y = []
    for edge in edge_list:
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])

    # Create edge trace
    edge_trace = go.Scatter(
        x=edge_x,
        y=edge_y,
        line=dict(width=0.5, color="#888"),
        hoverinfo="none",
        mode="lines",
    )

    # Create node trace
    node_trace = go.Scatter(
        x=x_nodes,
        y=y_nodes,
        mode="markers+text",
        hoverinfo="text",
        marker=dict(
            showscale=True,
            size=10,
            colorbar=dict(
                thickness=15,
                title="Node Connections",
                xanchor="left",
                titleside="right",
            ),
        ),
        text=[index_to_package[i] for i in range(len(pos))],
        textposition="top center",
    )

    # Create the figure
    fig = go.Figure(
        data=[edge_trace, node_trace],
        layout=go.Layout(
            showlegend=False,
            hovermode="closest",
            margin=dict(b=0, l=0, r=0, t=0),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        ),
    )
    fig.write_html("pypi.html")
    fig.show()
```

{% include pypi.html %}

<h2 style="text-align: center;">Conclusion</h2>
Through this comprehensive exploration, I've seen how GNNs can effectively model and analyze graph-structured data. The application in Python package analysis highlights the potential of GNNs in extracting meaningful insights from complex, interconnected data.

